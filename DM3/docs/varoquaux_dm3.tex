\documentclass{article}
%\usepackage[latin1]{inputenc}
\usepackage{graphicx,amssymb,amsmath,amsbsy,MnSymbol} % extensions pour maths
\usepackage{graphicx,mathenv}           % extensions pour figures
\usepackage[T1]{fontenc}        % pour les charactères accentués 
\usepackage[utf8]{inputenc} 
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{stmaryrd} % Pour les crochets d'ensemble d'entier
\usepackage{float}  % Pour placer les images là ou JE veux.

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\minimisons}{minimisons}
\DeclareMathOperator{\maximisons}{maximisons}
\DeclareMathOperator{\contraintes}{avec}


\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}
\setlength{\topmargin}{-0.4in}
\setlength{\topskip}{0.7in}    % between header and text
\setlength{\textheight}{9in} % height of main text
\setlength{\textwidth}{6in}    % width of text
\setlength{\oddsidemargin}{0in} % odd page left margin
\setlength{\evensidemargin}{0in} % even page left margin
%
%% Quelques raccourcis clavier :
\def\slantfrac#1#2{\kern.1em^{#1}\kern-.3em/\kern-.1em_{#2}}
\def\b#1{\mathbf{#1}}
\def\bs#1{\boldsymbol{#1}}
\def\m#1{\mathrm{#1}}
\bibliographystyle{acm}
%
\newcommand{\greeksym}[1]{{\usefont{U}{psy}{m}{n}#1}}
\newcommand{\inc}{\mbox{\small\greeksym{d}\hskip 0.05ex}}%
\pagenumbering{arabic}
\date{\today}
\title{Optimisation - DM1}
\author{Nelle Varoquaux}
\begin{document}
\maketitle

\section{Exercice 1}

Considérons le problème d'optimisation suivant:

\begin{equation*}
\min_{f \in \mathcal{H}_K, b \in \mathbb{R}} \{ \frac{1}{n} \sum_{i = 1}^n L
(f(x_i) + b,y_i) + \lambda \| f \|^2\}
\end{equation*}

où $\|f\|$ est la norme de $f$ dans le RKHS $\mathcal{H}_K$, et $L$ est
définie telle que:

\begin{equation*}
L(u, y) = \max(1 - uy, 0)^2  = \begin{cases}
			      0 & \mbox{si   } $uy$ \geq 0 \\
			      (1 - uy)^2 & \mbox{sinon}
			      \end{cases}
\end{equation*}

On a ici un problème d'optimisation convexe, mais non différentiable.
Introduisons les variables de relâchement $\xi_1, \xi_2, \dots, \xi_n$.
Le problème se reformule alors par:

\begin{align*}
\minimisons &  & \frac{1}{n} \sum_{i = 1}^n \xi_i^2 + \lambda \| f \|^2 \\
\contraintes & & \begin{cases}
		  \xi_i \geq (1 - y_i (f(x_i) + b)) \\
		  \xi_i \geq 0
		 \end{cases}
\end{align*}

On a de plus:

\begin{equation*}
\hat{f}(x) = \sum_{i = 1}^n \alpha_i K(x_i, x)
\end{equation*}

En réécrivant le problème d'optimisation $\mathbf{\alpha}$ et $\mathbf{\xi}$,
nous obtenons le problème primal suivant:

\begin{align*}
\minimisons &  & \frac{1}{n} \sum_{i = 1}^n \xi_i^2 + \lambda \alpha^T
\mathbf{K} \alpha \\
\contraintes & & \begin{cases}
		  \xi_i  + y_i \sum_{j = 1}^n \alpha_j K(x_j, x_i) + y_i b - 1 \geq 0\\
		  \xi_i \geq 0
		 \end{cases}
\end{align*}

On peut alors écrire le lagrangien:

\begin{align*}
\mathcal{L}(\alpha, \xi, \mu, \nu) = \frac{1}{n} \sum_{i = 1}^n \xi_i^2 +
\lambda \alpha^T \mathbf{K} \alpha - \sum_{i = 1}^n \mu_i \(\xi_i  + y_i \sum_{j = 1}^n
\alpha_j K(x_j, x_i) + y_i b - 1 \) - \sum_{i = 1}^n \nu_i \xi_i
\end{align*}

En dérivant en fonction de $\alpha$, on obtient:

\begin{align*}
\nabla_{\alpha} \mathcal{L} & = & 2 \lambda K \alpha - K Y \mu \\
			    & = & K (2 \lambda \alpha  - Y \mu)
\end{align*}

$\nabla_{\alpha} \mathcal{L} = 0$ donne donc (à une constante près, qui ne
change pas la solution, et que l'on prends donc égale à 0)

\begin{equation*}
\alpha^*_i(\mu, \nu) = \frac{Y_i \mu_i}{2 \lambda}
\end{equation*}


Dérivons maintenant par rapport à $\xi$:

\begin{align*}
\nabla_{\xi} \mathcal{L}& = & \frac{2}{n} \xi - \mu - \nu
\end{align*}

$\nabla_{\alpha} \mathcal{L} = 0$ donne donc:

\begin{equation*}
\xi = n \frac{(\mu + \nu)}{2}
\end{equation*}

Dérivons maintenant par rapport à $b$:

\begin{align*}
\nabla_b \mathcal{L} & = & - \sum_{i = 1}^n Y_i \mu_i
\end{align*}

Il faut donc, pour que $\nabla_b \mathcal{L} = 0$ que:

\begin{align*}
\sum_{i = 1}^n y_i \mu_i = 0
\end{align*}

En faisant l'hypothèse que $\sum_{i = 1}^n y_i \mu_i = 0$, on obtient:

\begin{align*}
g & = & \frac{1}{n} \sum_{i = 1}^n ( n \frac{\mu_i + \nu_i}{2})^2 +
\lambda \(\frac{Y \mu}{2 \lambda}\)^T \mathbf{K} \frac{Y \mu}{2 \lambda} - \sum_{i = 1}^n \mu_i \(n \frac{\mu_i + \nu_i}{2}  + y_i \sum_{j = 1}^n
 \frac{Y_j \mu_j}{2 \lambda}K(x_j, x_i) + y_i b - 1 \) - \sum_{i = 1}^n \nu_i
 n \frac{\mu_i + \nu_i}{2} \\
& = &  - \sum_{i = 1}^n n (\frac{\mu_i + \nu_i}{2})^2 - \frac{1}{4\lambda}\(Y
\mu\)^T \mathbf{K} Y \mu + \sum_{i = 1}^n \mu_i
\end{align*}

La fonction dual est donc:

\begin{align*}
g(\nu, \mu ) = \begin{cases}
		    - \sum_{i = 1}^n n (\frac{\mu_i + \nu_i}{2})^2 - \frac{1}{4\lambda}\(Y
\mu\)^T \mathbf{K} Y \mu + \sum_{i = 1}^n \mu_i & \mbox{si} \sum_{i = 1}^n y_i \mu_i = 0\\
		    - \infty & \mbox{sinon} 
		\end{cases}
\end{align*}

Et le problème dual:

\begin{align*}
\maximisons & & g(\mu, \nu) \\
\contraintes & &\mu \geq 0, \nu \geq 0, \sum_{i = 1}^n y_i \mu_i = 0
\end{align*}

On peut immédiatement voir que $\nu^* = 0$.

Le problème dual est donc équivalent à:

\begin{align*}
\max_{\mu | y^T \mu = 0} \sum_{i = 1}^n \mu_i - n \sum_{i = 1}^n
\frac{\mu_i^2}{4} - \sum_{i, j = 1}^n \frac{1}{4 \lambda}y_i y_j \mu_i \mu_j K(x_i, x_j)
\end{align*}

Le problème primal donne la solution suivante:

\begin{equation*}
\alpha_i(\mu, \nu) = \frac{Y_i \mu_i}{2 \lambda}
\end{equation*}

En réinjectant le problème dual dans cette équation, on obtiens le problème
équivalent suivant:

\begin{equation*}
\max _{\mu | y^T \mu = 0} \sum_{i = 1}^n 2\alpha_i y_i - n \sum_{i = 1}^n
\lambda \alpha_i^2 - \sum_{i, j = 1}^n \alpha_i \alpha_j K(x_i, x_j)
\end{equation*}

\begin{equation*}
\max _{\mu | y^T \mu = 0} 2 y \alpha - n \lambda \alpha^T \alpha -  \alpha^T K \alpha
\end{equation*}

On retrouve le problème 1-SVM avec une pénalité sur la taille de $\alpha$

\section{Exercice 2}

Nous avons ici des données de 15 classes différentes, composées de 90
descripteurs chacunes. Nous voulons utiliser un SVM pour les classifier. Or,
celui-ci est binaire.

Il existe plusieurs options pour utiliser un classifieur binaire pour du
multi-classe:

\begin{itemize}
\item one vs one: il faut entrainer 15 * 14 classifieurs.
\item one vs all: on entraine 15 classifieurs, qui correspondent à chacune des
classes, et prenons la probabilité la plus élevée pour attribuer un élément à
une classe.
\item dichotomie: on entraine un classifieur pour la premiere classe contre
toutes les autres, puis un classifieur pour la deuxième classe contre les
classes restantes etc...
\end{itemize}

J'ai choisi d'utiliser la méthode one-vs-all.

\begin{figure}
\includegraphics[width=300px]{linear.png}
\caption{Noyau linéaire}
\end{figure}

\begin{figure}
\includegraphics[width=300px]{rbf.png}
\caption{Noyau gaussien}
\end{figure}

\begin{figure}
\includegraphics[width=300px]{poly.png}
\caption{Noyau polynomial}
\end{figure}

On peut observer que sur tous les noyaux, lorsque $C$ augment, le taux de vrai
positif sur les données de classification augment. Cependant, le taux de vrai
positif sur les données d'entraînement fait une courbe en cloche. Le début de
la courbe correspond à de l'underfitting, et la fin de la courbe à de
l'overfitting.

Les résultats sont assez sensibles à la séparation en deux groupes des données
initiales: en effet, nous avons ici assez peu de données d'entraînement, et un
grand nombre de classe. Selon le mélange initial des données, le nombre de
données d'entraînement selon les classes peut varier énormement, ce qui a un
impact sur la qualité des classifieurs.

Le noyau linéaire est celui qui donne les moins bons résultats, une fois que
le paramètre $C$ est bien choisi. Cependant, c'est aussi celui qui est le
moins impacté par celui-ci: on observe que l'écart type est petit.

Le noyau gaussien et le noyau polynomial donne tout les deux des résultats
catastrophiques lorsque $C$ est petit, et de bon résultat, une fois que
celui-ci est bien choisi. On obtient environ $85\%$ de vrai positif.

\end{document}
